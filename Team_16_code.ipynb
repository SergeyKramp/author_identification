{"cells":[{"cell_type":"markdown","metadata":{"id":"n3OVD-TGuVxq"},"source":["## Setting up Google Drive"]},{"cell_type":"markdown","metadata":{"id":"alzTAmv0ul_I"},"source":["You can set up a connection to your google drive with the code below. This will create a prompt to connect the Colab notebook to your Google account.\n","\n","In order to access the files in our shared \"ML Team 16\"  folder, were I put the json files, you need to have a link to the folder in your personal Drive. To do this just go to \"Shared with Me\" and drag the folder over to \"My Drive\". This should do the trick."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18358,"status":"ok","timestamp":1667990061451,"user":{"displayName":"Sergey Kramp","userId":"02857214946885804529"},"user_tz":-60},"id":"hS0zf3CZsNVv","outputId":"e3793cd1-4004-43a2-a921-a098bc927b39"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /Drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/Drive')"]},{"cell_type":"markdown","metadata":{"id":"tUTwGHybvIJx"},"source":["After you run the above code, on the left under the \"Files\" category you should be able to see a folder called \"Drive\". This folder is your personal Google drive. Navigate to the \"ML_Team_16\" folder, and then to \"Data\" and right-click on the file you need to get the path to that file. You can paste this path in the read_json() function to read the json file with pandas and create a DataFrame object out of it."]},{"cell_type":"markdown","metadata":{"id":"Ljxwdmqlwyrs"},"source":["## Feaute Engineering"]},{"cell_type":"markdown","metadata":{"id":"tSecBK0l8Yp1"},"source":["### Loading the data"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":1417,"status":"ok","timestamp":1667990117514,"user":{"displayName":"Sergey Kramp","userId":"02857214946885804529"},"user_tz":-60},"id":"z-yZwtv-qC1p"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","articles = pd.read_json('/Drive/MyDrive/ML_Team_16/Data/train.json')\n"]},{"cell_type":"markdown","metadata":{"id":"YXmmyO348gB0"},"source":["### Cleaning the data"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":317,"status":"ok","timestamp":1667990918989,"user":{"displayName":"Sergey Kramp","userId":"02857214946885804529"},"user_tz":-60},"id":"XcfuJiUt8iOF","outputId":"b0f0e42c-f536-46b7-a943-91bff7bdfbe5"},"outputs":[{"name":"stdout","output_type":"stream","text":["paperId         0\n","title           0\n","authorId        0\n","authorName      0\n","abstract        0\n","year            0\n","venue         242\n","dtype: int64\n"]}],"source":["# Replacing blank entries with NaN\n","articles = articles.mask(articles == '')\n","\n","# Changing to authorId column to string\n","articles['authorId'] = articles['authorId'].astype('string')\n","\n","# Counting the number of NaNs in the dataset. There are 242 missing venue values.\n","print(articles.isna().sum())"]},{"cell_type":"markdown","metadata":{"id":"2EsdypXLIOwJ"},"source":["\n","### Tokenize and lemmatize abstract and title"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7885,"status":"ok","timestamp":1667991010919,"user":{"displayName":"Sergey Kramp","userId":"02857214946885804529"},"user_tz":-60},"id":"X-63REeuAX7L","outputId":"192b217c-9e7b-476b-d6be-6276065f4b0b"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]}],"source":["import spacy\n","import nltk\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","from nltk import WordNetLemmatizer\n","from spacy.lang.en import English\n","nlp = English()\n","tokenizer = nlp.tokenizer \n","lemmatizer = WordNetLemmatizer()"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":14491,"status":"ok","timestamp":1667991128839,"user":{"displayName":"Sergey Kramp","userId":"02857214946885804529"},"user_tz":-60},"id":"vxi0GzxepXQ4"},"outputs":[],"source":["tokens_all = []\n","\n","for _, row in articles.iterrows():\n","    tokens = tokenizer(row['title'].lower() + ' ' + row['abstract'].lower())\n","    lemmas = [lemmatizer.lemmatize(token.text) for token in tokens]\n","    lemmas_per_row = []\n","    \n","    for token, lemma in zip(tokens, lemmas):\n","    # removing stop words\n","      if token.is_stop:\n","        pass\n","      else:\n","        # storing the lemma of the token\n","        lemmas_per_row.append(lemma)\n","\n","    tokens_all.append(lemmas_per_row)\n","    \n","articles['tokens'] = tokens_all"]},{"cell_type":"markdown","metadata":{"id":"WHZY62EkIYYH"},"source":["### Train Test Split"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":382,"status":"ok","timestamp":1667991159384,"user":{"displayName":"Sergey Kramp","userId":"02857214946885804529"},"user_tz":-60},"id":"yG5btY_TINCA"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","train, test = train_test_split(articles, test_size=0.1, random_state=32)"]},{"cell_type":"markdown","metadata":{"id":"dEUPM37t5lRt"},"source":["### Functions for creating n-grams"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1667991161890,"user":{"displayName":"Sergey Kramp","userId":"02857214946885804529"},"user_tz":-60},"id":"Zalzo0U4FcuW"},"outputs":[],"source":["def character_ngrams(s, n):\n","    \n","    \"\"\"takes in a string and an integer defining the size of ngrams.\n","     Returns the character ngrams of desired size in the input string\"\"\"\n","    \n","    s = '#'*(n-1) + s.replace('ยง', ' ') + '#'*(n-1)\n","    ngrams = [s[i:i+n] for i in range(len(s)-n+1)]\n","    \n","    return ngrams\n","\n","\n","def word_ngrams(l, n):\n","    \n","    \"\"\"takes in a list and an integer defining the size of ngrams.\n","     Returns the word ngrams of desired size in the input string\"\"\"\n","    \n","    \n","    s = ['#']*(n-1) + l + ['#']*(n-1)\n","    ngrams = ['ยง'.join(s[i:i+n]) for i in range(len(s)-n+1)]\n","    \n","    return ngrams\n","\n","\n","def token2ngrams(articles, n, char_n_grams=False):\n","\n","    featurised_articles = []\n","\n","    for i, row in articles.iterrows():\n","        \n","        featurised_row = []\n","\n","        if char_n_grams:\n","          featurised_row.extend(character_ngrams(row['tokens'], n))\n","        else:\n","          featurised_row.extend(word_ngrams(row['tokens'], n))\n","        \n","        featurised_articles.append(featurised_row)\n","\n","\n","    articles['ngrams'] = featurised_articles\n","    \n","    return articles\n","\n","\n","def feature_matrix(articles, mapping=None):\n","    \n","    if not mapping:\n","        all_ngrams = {}\n","        for _, row in articles.iterrows():\n","          for ngram in row['ngrams']:\n","            try:\n","              all_ngrams[ngram] += 1\n","            except KeyError:\n","              all_ngrams[ngram] = 1\n","\n","        # removing ngrams that appear 5 times or less \n","        reduced_ngrams = set([ngram for ngram, count in all_ngrams.items() if count > 5])\n","        mapping = {ngram: i for i, ngram in enumerate(reduced_ngrams)}\n","    \n","    X = np.zeros((len(articles.index), len(mapping)), dtype='uint8')\n","    #y = np.zeros(len(articles.index))\n","    y = []\n","\n","    r = 0\n","    for _, row in articles.iterrows():\n","        #y[r] = row['authorId']\n","        y.append(str(row['authorId']))\n","        for ngram in row['ngrams']:\n","            try:\n","                X[r, mapping[ngram]] += 1\n","            except KeyError:\n","                pass\n","        r += 1\n","    \n","    return X, y, mapping\n","\n","# Credit for these functions goes to Dr. Giovanni Cassanni (https://www.tilburguniversity.edu/staff/g-cassani)\n"]},{"cell_type":"markdown","metadata":{"id":"XIuUV3Jd6H5c"},"source":["### Creating n-grams for training and testing datasets"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":1354,"status":"ok","timestamp":1667991169623,"user":{"displayName":"Sergey Kramp","userId":"02857214946885804529"},"user_tz":-60},"id":"SFOSGskRHY6-"},"outputs":[],"source":["train = token2ngrams(train, n=1, char_n_grams=False)\n","test = token2ngrams(test, n=1, char_n_grams=False)"]},{"cell_type":"markdown","metadata":{"id":"tvfBx4ThSgLt"},"source":["### Creating feature and target matrices"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k_tk0fjN3tSj"},"outputs":[],"source":["X_train, y_train, ngram2id = feature_matrix(train)\n","X_test, y_test, _ = feature_matrix(test, mapping=ngram2id)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"88ehSOVzPlBl","outputId":"39239fda-3cd5-4513-8f00-ab5f40484487"},"outputs":[{"name":"stdout","output_type":"stream","text":["(10916, 7101)\n","(1213, 7101)\n"]}],"source":["print(X_train.shape)\n","print(X_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"kkPrIE5OSlSO"},"source":["### Doing feature selection using chi squared"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ngpC48lkSyAz"},"outputs":[],"source":["from sklearn.naive_bayes import MultinomialNB\n","from sklearn.feature_selection import SelectKBest, chi2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_4tsw17VLcDP","outputId":"dbce5282-5e38-4b37-ede4-8e6bad0e242c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Done\n"]}],"source":["\"In principle this should not be used, unless a feature set of different size is required. For the data import it as written down below\"\n","\n","#selector = SelectKBest(chi2, k=500).fit(X_train, y_train)\n","#X_train_reduced = selector.transform(X_train)\n","#X_test_reduced = selector.transform(X_test)\n","\n","#print('Done')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jItWb5T0oxoZ"},"outputs":[],"source":["#If new data is generated, save it using the following code\n","X_train_reduced.tofile(\"x_train_reduced_randomstate32.csv\", sep=\",\")\n","X_test_reduced.tofile(\"x_test_reduced_randomstate32.csv\", sep=\",\")"]},{"cell_type":"markdown","metadata":{"id":"8EE5mpcSoxob"},"source":["### Opening feature-reduced training and test sets\n","\n","Reduced data for chi2 of 500 is saved down below; open this data from the folder. This is faster than running the code above which takes at least 40 minutes. \n","Use the code below to open both files as Pandas dataframes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vf6Xjs08oxoc"},"outputs":[],"source":["X_train_reduced = pd.read_csv('x_train_reduced_randomstate32.csv', delimiter=\",\", header=None)\n","X_test_reduced = pd.read_csv('x_test_reduced_randomstate32.csv', delimiter=\",\", header=None)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f59VU5zwoxod"},"outputs":[],"source":["selector.get_feature_names_out()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ytEy_Pwg6IT"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score"]},{"cell_type":"markdown","metadata":{"id":"E58Svhayoxof"},"source":["### Training NB with Full Feature Matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"14oXVp3P6Hrm","outputId":"31821152-e3cd-44a9-82d7-1d7e28ab7aac"},"outputs":[{"data":{"text/plain":["MultinomialNB(alpha=0.001)"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["NB = MultinomialNB(alpha=0.001, fit_prior=True)\n","NB.fit(X_train, y_train)\n","\n","#Highest accuracy of 10.8%\""]},{"cell_type":"markdown","metadata":{"id":"9hwHwQKroxoh"},"source":["### Training NB with Full Feature Matrix & Year included"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZC_BMGo7oxoj"},"outputs":[],"source":["X_train_years = X_train \n","X_train_years = np.hstack((X_train_years, np.reshape(train['year'].values,(-1,1)))) #a column of year is added to X_train\n","\n","X_test_years = X_test\n","X_test_years = np.hstack((X_test_years, np.reshape(test['year'].values,(-1,1)))) #a column of year is added to X_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jQsZmhh6oxok","outputId":"bdcd61b7-46af-44a5-f808-fa765a0fd5e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.10305028854080792\n"]}],"source":["NB_full_years = MultinomialNB(alpha=0.01, fit_prior=True)\n","NB_full_years.fit(X_train_years, y_train)\n","y_pred = NB_full_years.predict(X_test_years)\n","print(accuracy_score(y_test, y_pred))\n","\n","#accuracy outcome: 0.10305028854080792"]},{"cell_type":"markdown","metadata":{"id":"efkqGAi4oxom"},"source":["### Training NB with Reduced Feature Matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iTsIFxGsoxon","outputId":"6908cbf7-ebc5-4352-8345-016fa3cbfd7d"},"outputs":[{"data":{"text/plain":["MultinomialNB(alpha=0.001)"]},"execution_count":104,"metadata":{},"output_type":"execute_result"}],"source":["NB_reduced = MultinomialNB(alpha=0.001, fit_prior=True)\n","NB_reduced.fit(X_train_reduced, y_train)\n"]},{"cell_type":"markdown","metadata":{"id":"f4KohBGtoxoo"},"source":["### Training NB with Reduced features and Year-features\n","This does not give higher accuracy scores compared with only using reduced features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cNfXpfNJoxop"},"outputs":[],"source":["#Testing with the inclusion of year-feature\n","temporary = X_train_reduced \n","temporary = np.hstack((temporary, np.reshape(train['year'].values,(-1,1)))) #a column of year is added to X_train_reduced\n","\n","temporary_test = X_test_reduced\n","temporary_test = np.hstack((temporary_test, np.reshape(test['year'].values,(-1,1)))) #a column of year is added to X_test_reduced\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sz-wAsoIoxor","outputId":"1dd6c4a6-b575-4dd2-b37f-5d79e74d68ca"},"outputs":[{"data":{"text/plain":["MultinomialNB(alpha=0.001)"]},"execution_count":102,"metadata":{},"output_type":"execute_result"}],"source":["NB_reduced_and_year = MultinomialNB(alpha=0.001, fit_prior=True)\n","NB_reduced_and_year.fit(temporary, y_train)\n","\n","y_pred_reduced = NB_reduced_and_year.predict(temporary_test)\n","print(accuracy_score(y_test, y_pred_reduced))\n","#Accuracy of 0.016488046166529265"]},{"cell_type":"markdown","metadata":{"id":"ucC7P4p3oxos"},"source":["### Accuracy Score with Full Matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FURzBTkRb39h","outputId":"b3cac634-8ca2-4f55-991e-04b70228d481"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.1079967023907667\n","[0. 0. 0. ... 0. 0. 0.]\n","[0. 0. 0. ... 0. 0. 0.]\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Bas Rongen\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\Bas Rongen\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["y_pred = NB.predict(X_test)\n","print(accuracy_score(y_test, y_pred))\n","print(precision_score(y_test, y_pred, average=None))\n","print(recall_score(y_test, y_pred, average=None))\n","\n","#Accuracy 0.1079967023907667"]},{"cell_type":"markdown","metadata":{"id":"GikkLM9Xoxou"},"source":["### Accuracy Score with Reduced Matrix "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K6AKFUmloxou","outputId":"708ab601-ed5e-4b13-f5c1-c11e90bf8731"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.01731244847485573\n"]}],"source":["y_pred_reduced = NB_reduced.predict(X_test_reduced)\n","print(accuracy_score(y_test, y_pred_reduced))\n","#print(precision_score(y_test, y_pred_reduced, average=None))\n","#print(recall_score(y_test, y_pred_reduced, average=None))"]},{"cell_type":"markdown","metadata":{"id":"nMdUaJijoxov"},"source":["## Logistic Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DrRY603Xoxow"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"53xgAoMPoxow"},"outputs":[],"source":["#The following code turns the data into dataframes and extends it with the \"year\" feature\n","X_train_df = pd.DataFrame(X_train)\n","X_test_df = pd.DataFrame(X_test)\n","\n","X_train_reduced_df = pd.DataFrame(X_train_reduced)\n","X_test_reduced_df = pd.DataFrame(X_test_reduced)\n","\n","X_train_df['year'] = train['year'].astype(\"int\")\n","X_test_df['year'] = test['year'].astype(\"int\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bX2wDcNUoxox","outputId":"c6df33c7-e042-47ba-9ab7-f5e487f1200c"},"outputs":[{"name":"stdout","output_type":"stream","text":["(10916, 7102)\n"]},{"data":{"text/plain":["(10916, 7101)"]},"execution_count":129,"metadata":{},"output_type":"execute_result"}],"source":["X_train_years = X_train \n","X_train_years = np.hstack((X_train_years, np.reshape(train['year'].values,(-1,1)))) #a column of year is added to X_train\n","\n","X_test_years = X_test\n","X_test_years = np.hstack((X_test_years, np.reshape(test['year'].values,(-1,1)))) #a column of year is added to X_test"]},{"cell_type":"markdown","metadata":{"id":"WUpnKaraoxoz"},"source":["### Replacing missing values of year by median year (2018)"]},{"cell_type":"markdown","metadata":{"id":"JrfvmIo6oxo0"},"source":["I am not sure whether we need to actually need to impute these missing year values. Because there shouldn't be any missing. Correct me if I am wrong, but running an \"is.null().sum()\", on the whole train data from year, does not show that any values are missing. As such the X_train dataframe shouldn't have missing values either"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n1nDHO3Toxo0","outputId":"5f9fe8fc-8875-4d44-ce25-f0612e0b439c"},"outputs":[{"data":{"text/plain":["0"]},"execution_count":147,"metadata":{},"output_type":"execute_result"}],"source":["train[\"year\"].isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3biTBHc3oxo1"},"outputs":[],"source":["X_train_df['year'] = X_train_df['year'].fillna(2018)\n","X_test_df['year'] = X_test_df['year'].fillna(2018)\n","\n","X_train_reduced_df['year'] = X_train_df['year'].fillna(2018)\n","X_test_reduced_df['year'] = X_test_df['year'].fillna(2018)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1pZDCNGWoxo1"},"outputs":[],"source":["logreg = LogisticRegression(penalty='none')\n","logreg.fit(X_train_reduced_df, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zpzkapD8oxo2"},"outputs":[],"source":["logreg_reduced = logreg"]},{"cell_type":"markdown","metadata":{"id":"8arlgJxxoxo2"},"source":["### Accuracy Score Logistic Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-wsSYYHQoxo3","outputId":"2b916812-c106-47c3-eef5-562e834789c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.01483924154987634\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Bas Rongen\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n","  warnings.warn(\n"]}],"source":["y_pred_reduced_log = logreg_reduced.predict(X_test_reduced_df)\n","print(accuracy_score(y_test, y_pred_reduced_log))"]},{"cell_type":"markdown","metadata":{"id":"naGsfA0soxo3"},"source":["### GridSearch - Testing different parameters for logistic regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"38nTfVProxo4","outputId":"5d6634ec-4218-44d9-9cd0-c2c28a971024"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Bas Rongen\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n","  warnings.warn(\n"]}],"source":["from sklearn.model_selection import GridSearchCV\n","\n","grid_reduced = GridSearchCV(estimator=LogisticRegression(),\n","        param_grid={'C': [10, 1, 0.1, 0.01], 'penalty': ['none', 'l1', 'l2', 'elasticnet'], 'solver' : [\"saga\"]}, n_jobs=-1, cv=1)\n","grid_reduced.fit(X_train_reduced_df, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sBiE_SoFoxo4"},"outputs":[],"source":["print(grid_reduced.best_estimator_)\n","print('best score: ' + str(round(grid.best_score_,2)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c-HnlBzeoxo5"},"outputs":[],"source":["grid = GridSearchCV(estimator=LogisticRegression(),\n","        param_grid={'C': [10, 1, 0.1, 0.01], 'penalty': ['none', 'l1', 'l2', 'elasticnet'], 'solver' : [\"saga\"]}, n_jobs=-1, cv=1)\n","grid.fit(X_train_df, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## some random stuff"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"19e74d83289f73bcd3a18c224f2d83435345cd709064a0a23963fbf60b82a8c3"}}},"nbformat":4,"nbformat_minor":0}
